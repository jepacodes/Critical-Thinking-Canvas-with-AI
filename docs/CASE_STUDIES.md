# Case Studies

These are examples of people using the framework on real tasks. They include the complications and adjustments—not just the clean version after the fact.

---

## Case Study 1: The Quarterly Report

**Who:** Rachel, a product manager at a SaaS company

**The task:** Write a quarterly analysis of feature usage and recommend what to prioritize next quarter.

### The Problem

Rachel had gotten into a pattern. Every quarter, she'd export the analytics data, paste it into ChatGPT, and ask for a summary with recommendations. She'd review what it produced, make a few edits, and send it to leadership.

Fast forward six months: in a strategy meeting, someone asked why the product team had deprioritized feature X. Rachel couldn't remember. She looked back at her reports and saw she'd recommended exactly that—but she couldn't explain the reasoning anymore. It was somewhere in her process with ChatGPT, but she hadn't retained it.

That's when she realized she'd outsourced her judgment and forgotten to keep a copy.

### Working Through the Framework

**AI Decision Canvas:**

Rachel ran through the Three Questions honestly and realized she was failing all three:

1. **Material engagement?** She wasn't really looking at the data herself. She was looking at AI's summary of the data.

2. **Productive resistance?** AI wasn't challenging her. It was just producing what she asked for.

3. **Metacognition?** She had no record of her reasoning. She couldn't explain past decisions.

**Redesigned approach:**

- Spend 2 hours with the raw analytics before involving AI
- Form her own initial thesis about what's happening
- Use AI to challenge her interpretation and find what she missed
- Keep a decision log explaining key recommendations

### What Changed

The next quarter, she did it differently.

First, she spent a morning with the data. No AI. Just her and the numbers. She made notes: "Feature Y adoption is way lower than expected. Is this a discoverability problem or a usefulness problem? Need to check if people who find it actually use it."

Then she brought in AI—but differently. Instead of "summarize and recommend," she asked:

- "What might explain low adoption of feature Y other than discoverability?"
- "Here's my interpretation. What's wrong with it?"
- "What am I probably missing in this data?"

The conversation was harder. AI pushed back on some of her assumptions. She had to defend her reasoning, which made her sharpen it.

Her final report included a decision log: "Recommended deprioritizing feature X because usage dropped 40% and support tickets suggest users find it confusing. Alternative considered: maybe users just need better onboarding. Rejected because trial-period usage was actually higher than post-trial—confusion develops over time, not immediately."

### Six Months Later

In a strategy meeting, someone asked about feature X again. This time, Rachel pulled up the decision log. "Here's why—usage data showed this, we considered alternatives, here's why we went this way."

She owned the decision because she'd actually made it.

### Her Reflection

"The first approach was faster. The second approach was maybe 3 hours more work. But the difference in understanding was huge. I actually know our product now, not just what I asked AI to tell me about it."

---

## Case Study 2: The Research Summary

**Who:** David, a policy analyst at a think tank

**The task:** Synthesize 15 research papers on housing policy for a briefing document.

### The Problem

David's job involves reading a lot of research and synthesizing it for policymakers. AI seemed perfect for this—it could summarize papers quickly and help him cover more ground.

But he noticed something: his briefings were getting flatter. They covered more papers, but said less. The insights that used to emerge from deep reading—the unexpected connections, the subtle contradictions between papers—weren't there anymore.

Also, he was starting to get nervous. What if someone asked about a paper he'd only read the AI summary of? He was building briefings on foundations he hadn't actually examined.

### Working Through the Framework

**Task Analysis Canvas:**

David got specific about what he was actually trying to accomplish:

- **Goal:** Help policymakers understand the state of evidence on rent control. Not just "what do studies say" but "what should a thoughtful person conclude from this body of evidence?"

- **Materials:** 15 papers. But not all equally important. Maybe 4-5 are seminal, the rest are follow-ups or limited in scope.

- **Decisions I must make:** Which papers matter most. How to reconcile conflicting findings. What the weight of evidence actually suggests. What caveats matter.

- **My value add:** I know the policy context. I know what questions policymakers are actually asking. I can distinguish between statistically significant and practically significant.

**Strategic Engagement Canvas:**

- **Deep engagement:** The 5 most important papers. Read fully, take notes, form my own interpretation.
- **Moderate engagement:** The 10 other papers. Read abstracts and conclusions, use AI to help identify key findings, verify anything I cite.
- **Light engagement:** Formatting, citation management, boilerplate sections.

### What Changed

David triaged the papers first. Without AI, he read abstracts and identified which ones deserved full attention. (This itself was valuable—it made him think about what counts as important in this field.)

The 5 key papers, he read fully. He annotated them, argued with them in the margins, noticed where paper A contradicted paper B.

For the other 10, he used AI differently. "Given my notes on these key papers, what does this paper add? Does it challenge or support the main findings? What's its key limitation?"

When AI summarized a paper, he verified specific claims before including them. If a finding was going to matter for his conclusion, he went to the source.

### The Result

The briefing was shorter than it would have been if he'd tried to cover all 15 papers equally. But it was better. It had a clear argument. It identified genuine tensions in the evidence. It made recommendations that accounted for limitations.

A policymaker called with a question about a specific paper. David could answer because he'd actually read it.

### His Reflection

"I was trying to cover more ground, but covering ground isn't the job. Understanding deeply enough to give good advice—that's the job. AI can help me process more, but I still have to do the understanding myself."

---

## Case Study 3: The Business Case

**Who:** Maria, a senior consultant

**The task:** Write a business case for a client's proposed expansion.

### The Problem

Maria had 15 years of experience. She knew how to write a business case. The question was whether she could use AI to do it faster without losing the judgment that made her valuable.

Her first attempt was telling: she asked AI to draft the business case based on the client data. It produced something reasonable-looking—but Maria caught three places where the assumptions didn't match the client's actual situation. If she'd been rushing, she might have missed them.

That made her nervous. What else might she miss if she was just reviewing instead of writing?

### Working Through the Framework

**AI Decision Canvas:**

Maria was honest with herself:

1. **Material engagement?** Mixed. She knew the client well and had the context. But she wasn't really engaging with constructing the argument.

2. **Productive resistance?** No. AI was just executing her request.

3. **Metacognition?** Barely. She was in review mode, not thinking mode.

The question that stuck with her: "Would I be comfortable presenting this as my work?" Technically, yes—she'd reviewed it. But actually, she wasn't sure she could defend every assumption if challenged.

**Thought Architecture Canvas:**

She decided to build the business case herself and use AI differently:

- Outline the argument structure herself
- Identify the key assumptions and stress-test them
- Write the core sections (executive summary, recommendation) herself
- Use AI for supporting sections after the structure is set
- Keep a record of assumptions and why she chose them

### What Changed

She started with a blank page and her outline. What's the core question? What are the options? What criteria matter for choosing? She thought it through herself.

Then she brought in AI—but for specific things:
- "Here's my financial model. What's wrong with it?"
- "What are the three biggest risks I might be underweighting?"
- "How would a skeptic attack this recommendation?"

When AI found a real issue, she engaged with it. When it raised concerns that didn't apply to this client's situation, she articulated why and kept her approach.

The final document took longer than the AI-draft approach would have. Maybe 30% longer. But she could defend every assumption. She knew where the soft spots were. She'd thought about the alternatives and could explain why she'd rejected them.

### The Client Meeting

The client asked a challenging question about a key assumption. Maria walked them through her reasoning—the alternatives she'd considered, why this assumption fit their situation, what would change if they were wrong.

Later, the client told her: "We've worked with consultants who just give us AI-polished deliverables. You actually think about our situation."

### Her Reflection

"The question I keep asking myself is: what am I being paid for? If it's just the document, AI can produce that. But I'm being paid for judgment. For thinking through their specific situation. That's what I can't outsource."

---

## Case Study 4: The Learning Challenge

**Who:** James, a marketing director moving into a new industry

**The task:** Get up to speed on the healthcare industry's regulatory environment.

### The Problem

James was new to healthcare. He had a lot to learn quickly. AI seemed like a perfect way to accelerate his learning.

But three months in, he realized something troubling. He could talk about healthcare regulations in meetings—he knew the right terms, the general landscape. But when someone pushed on specifics, he floundered. His knowledge was a mile wide and an inch deep.

He'd learned about healthcare regulations. He hadn't learned healthcare regulations.

### Working Through the Framework

**Task Analysis Canvas:**

- **Goal:** Actually understand the regulatory environment well enough to make good decisions and advise the team competently.

- **Domain Assessment:** This is outside my domain. I'm learning, not applying existing expertise.

The framework's guidance for learning outside your domain was clear: use AI to learn, not to shortcut learning. Engage heavily with source materials. Be extra skeptical because you can't spot errors.

**Strategic Engagement Canvas:**

James realized he'd been in "green zone" mode for everything—light engagement, AI doing the heavy lifting. For learning, he needed to flip that.

- **Deep engagement:** Primary regulatory documents. Key case studies of compliance failures. Foundational concepts.
- **Moderate engagement:** Industry commentary and analysis. AI helping him understand what he was reading.
- **Light engagement:** Only for administrative stuff—finding documents, organizing notes.

### What Changed

He went back to basics. Actually read the key regulations. Not summaries—the actual text. It was painful. He had to look up terms constantly. He had to reread paragraphs.

But he started understanding *why* regulations were structured the way they were. He could see the logic. When AI had told him "X is required for Y," he now understood the reasoning behind that requirement.

He used AI differently too:
- "I just read Section 1128B of the Social Security Act. Explain it to me like I'm a smart person who's new to this."
- "What's a common misunderstanding about this regulation that I should avoid?"
- "Give me three examples of companies that got this wrong and why."

When AI explained something, he went back to the source to verify. This was slower, but he actually retained it.

### Six Months Later

In a meeting about a potential campaign, James noticed something: "Wait, that approach might run into issues with Stark Law. Let me check." He could engage substantively because he'd actually learned the material.

A colleague mentioned that James seemed more confident lately. "You used to hedge everything about the regulatory stuff. Now you actually have opinions."

### His Reflection

"AI is a great teacher's aide, but it's not a shortcut to actually knowing things. The painful process of reading the primary sources and struggling with them—that's how learning works. AI can help me understand what I'm reading, but it can't do the reading for me."

---

## Common Patterns

Across these cases, a few patterns emerge:

**The initial approach was faster but shallower.** Every person started by using AI to accelerate the work. It worked for producing outputs, but created gaps in understanding.

**The framework forced an honest self-assessment.** The Three Questions don't let you pretend you're thinking when you're not. Each person had to admit where they'd disengaged.

**Redesigning the approach added time but added more value.** The new approaches took 20-50% longer. But they produced better work and built capabilities rather than eroding them.

**The long-term benefit was clear.** When challenged later, the people who'd engaged deeply could defend their work. The ones who'd outsourced their thinking were exposed.

**It's not about using AI less.** Every person kept using AI. They just used it differently—for challenge instead of production, for verification instead of replacement.


